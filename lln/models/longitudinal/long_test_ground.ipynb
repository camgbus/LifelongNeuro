{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of subjects and time points\n",
    "num_subjects = 100\n",
    "time_points = ['T1', 'T2', 'T3']\n",
    "\n",
    "# Function to categorize blood pressure\n",
    "def categorize_bp(systolic_bp):\n",
    "    if systolic_bp < 120:\n",
    "        return 0 #'Normal'\n",
    "    elif 120 <= systolic_bp <= 129:\n",
    "        return 1 #'Elevated'\n",
    "    else:\n",
    "        return 2 #'Hypertension'\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    'SubjectID': np.repeat([f'subject_{i:03}' for i in range(1, num_subjects + 1)], len(time_points)),\n",
    "    'TimePoint': time_points * num_subjects,\n",
    "    'SystolicBP': np.random.randint(110, 150, num_subjects * len(time_points)),\n",
    "    'DiastolicBP': np.random.randint(70, 90, num_subjects * len(time_points))\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_longitudinal = pd.DataFrame(data)\n",
    "\n",
    "# Filter T3 time points to add the BP category\n",
    "df_longitudinal['BPCategory'] = df_longitudinal['SystolicBP'].apply(categorize_bp)\n",
    "\n",
    "# Calculate the number of rows to remove, to test whether it is robust to missing rows\n",
    "#num_rows_to_remove = int(len(df_longitudinal) * 0.2)\n",
    "#df_longitudinal = df_longitudinal.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#df_longitudinal = df_longitudinal.iloc[num_rows_to_remove:]\n",
    "\n",
    "all_subjects = list(df_longitudinal['SubjectID'].unique())\n",
    "train_subjects, test_subjects = all_subjects[:80], all_subjects[80:]\n",
    "df_train = df_longitudinal[df_longitudinal['SubjectID'].isin(train_subjects)]\n",
    "df_test = df_longitudinal[df_longitudinal['SubjectID'].isin(test_subjects)]\n",
    "\n",
    "dataset_path = \"C:\\\\Users\\\\camgonza\\\\Box\\\\Camila Gonzalez's Files\\\\Data\\\\NCANDA\\\\NCANDA_experiments\\\\long_testground\"\n",
    "df_longitudinal.to_csv(dataset_path + \"/df_longitudinal.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lln.data.pytorch.get_dataset import LongDataset\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lln.models.longitudinal.LSTM import LSTM\n",
    "from lln.training.LongitudinalTrainer import LongitudinalTrainer\n",
    "seq_to_seq = True\n",
    "input_dim = 2  # Number of features (systolic BP)\n",
    "hidden_dim = 100  # Number of hidden layers\n",
    "output_dim = 3  # Number of output classes (Normal, Elevated, Hypertension)\n",
    "n_layers = 1  # Number of LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject_001', 'subject_002', 'subject_003', 'subject_004', 'subject_005', 'subject_006', 'subject_007', 'subject_008', 'subject_009', 'subject_010', 'subject_011', 'subject_012', 'subject_013', 'subject_014', 'subject_015', 'subject_016', 'subject_017', 'subject_018', 'subject_019', 'subject_020', 'subject_021', 'subject_022', 'subject_023', 'subject_024', 'subject_025', 'subject_026', 'subject_027', 'subject_028', 'subject_029', 'subject_030', 'subject_031', 'subject_032', 'subject_033', 'subject_034', 'subject_035', 'subject_036', 'subject_037', 'subject_038', 'subject_039', 'subject_040', 'subject_041', 'subject_042', 'subject_043', 'subject_044', 'subject_045', 'subject_046', 'subject_047', 'subject_048', 'subject_049', 'subject_050', 'subject_051', 'subject_052', 'subject_053', 'subject_054', 'subject_055', 'subject_056', 'subject_057', 'subject_058', 'subject_059', 'subject_060', 'subject_061', 'subject_062', 'subject_063', 'subject_064', 'subject_065', 'subject_066', 'subject_067', 'subject_068', 'subject_069', 'subject_070', 'subject_071', 'subject_072', 'subject_073', 'subject_074', 'subject_075', 'subject_076', 'subject_077', 'subject_078', 'subject_079', 'subject_080']\n",
      "['subject_081', 'subject_082', 'subject_083', 'subject_084', 'subject_085', 'subject_086', 'subject_087', 'subject_088', 'subject_089', 'subject_090', 'subject_091', 'subject_092', 'subject_093', 'subject_094', 'subject_095', 'subject_096', 'subject_097', 'subject_098', 'subject_099', 'subject_100']\n"
     ]
    }
   ],
   "source": [
    "# Define PyTorch datasets and dataloaders\n",
    "datasets = OrderedDict([('Train', LongDataset(df_train, feature_cols=['SystolicBP', 'DiastolicBP'], target_col='BPCategory', seq_to_seq=seq_to_seq, id_col='SubjectID', seq_col='TimePoint', timepoints=['T1', 'T2', 'T3'])),\n",
    "            ('Test', LongDataset(df_test, feature_cols=['SystolicBP', 'DiastolicBP'], target_col='BPCategory', seq_to_seq=seq_to_seq, id_col='SubjectID', seq_col='TimePoint', timepoints=['T1', 'T2', 'T3']))])\n",
    "print(datasets['Train'].subjects)\n",
    "print(datasets['Test'].subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([10, 3, 2])\n",
      "Shape of y: torch.Size([10, 3]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 10\n",
    "dataloaders = OrderedDict([(dataset_name, DataLoader(dataset, batch_size=batch_size, shuffle=True))\n",
    "    for dataset_name, dataset in datasets.items()])\n",
    "for X, y in dataloaders['Train']:\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(2, 100, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "save_path = \"C:\\\\Users\\\\camgonza\\\\Box\\\\Camila Gonzalez's Files\\\\Data\\\\NCANDA\\\\NCANDA_experiments\\\\long_testground\\\\models\"\n",
    "model = LSTM(input_dim, hidden_dim, output_dim, save_path=save_path, nr_layers=n_layers, seq_to_seq=seq_to_seq)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and trainer\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "trainer_path = \"C:\\\\Users\\\\camgonza\\\\Box\\\\Camila Gonzalez's Files\\\\Data\\\\NCANDA\\\\NCANDA_experiments\\\\long_testground\\\\trainer\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trainer = LongitudinalTrainer(trainer_path, 'cpu', optimizer, loss_f, seq_to_seq=seq_to_seq, labels=[\"Normal\", \"Elevated\", \"Hypertension\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:59,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train CrossEntropyLoss: 1.165 B-Acc.: 0.318 F1: 0.135\n",
      "Test CrossEntropyLoss: 1.215 B-Acc.: 0.333 F1: 0.095\n",
      "Saved PyTorch model state LSTM_epoch0.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\models\n",
      "Saved trainer state LongitudinalTrainer_optimizer_epoch0.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\\states\n",
      "Progress stored in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\n",
      "Ending epoch 1, loss 1.1023691147565842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:01<00:06, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "Train CrossEntropyLoss: 0.702 B-Acc.: 0.570 F1: 0.517\n",
      "Test CrossEntropyLoss: 0.631 B-Acc.: 0.567 F1: 0.553\n",
      "Ending epoch 11, loss 0.7000902071595192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:02<00:09,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "Train CrossEntropyLoss: 0.621 B-Acc.: 0.636 F1: 0.611\n",
      "Test CrossEntropyLoss: 0.568 B-Acc.: 0.638 F1: 0.626\n",
      "Saved PyTorch model state LSTM_epoch20.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\models\n",
      "Saved trainer state LongitudinalTrainer_optimizer_epoch20.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\\states\n",
      "Progress stored in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\n",
      "Ending epoch 21, loss 0.6246859803795815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:03<00:06, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30\n",
      "Train CrossEntropyLoss: 0.587 B-Acc.: 0.648 F1: 0.628\n",
      "Test CrossEntropyLoss: 0.540 B-Acc.: 0.638 F1: 0.636\n",
      "Ending epoch 31, loss 0.6089584305882454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:04<00:06,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40\n",
      "Train CrossEntropyLoss: 0.561 B-Acc.: 0.663 F1: 0.656\n",
      "Test CrossEntropyLoss: 0.519 B-Acc.: 0.666 F1: 0.696\n",
      "Saved PyTorch model state LSTM_epoch40.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\models\n",
      "Saved trainer state LongitudinalTrainer_optimizer_epoch40.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\\states\n",
      "Progress stored in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\n",
      "Ending epoch 41, loss 0.5725197978317738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:04<00:05,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Train CrossEntropyLoss: 0.537 B-Acc.: 0.689 F1: 0.690\n",
      "Test CrossEntropyLoss: 0.506 B-Acc.: 0.685 F1: 0.706\n",
      "Ending epoch 51, loss 0.5577188208699226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:05<00:03, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60\n",
      "Train CrossEntropyLoss: 0.528 B-Acc.: 0.675 F1: 0.684\n",
      "Test CrossEntropyLoss: 0.499 B-Acc.: 0.694 F1: 0.730\n",
      "Saved PyTorch model state LSTM_epoch60.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\models\n",
      "Saved trainer state LongitudinalTrainer_optimizer_epoch60.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\\states\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:06<00:06,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress stored in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\n",
      "Ending epoch 61, loss 0.5305501036345959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:07<00:02,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70\n",
      "Train CrossEntropyLoss: 0.508 B-Acc.: 0.647 F1: 0.640\n",
      "Test CrossEntropyLoss: 0.484 B-Acc.: 0.683 F1: 0.721\n",
      "Ending epoch 71, loss 0.5071765743196011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:07<00:01, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80\n",
      "Train CrossEntropyLoss: 0.484 B-Acc.: 0.746 F1: 0.751\n",
      "Test CrossEntropyLoss: 0.480 B-Acc.: 0.732 F1: 0.757\n",
      "Saved PyTorch model state LSTM_epoch80.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\models\n",
      "Saved trainer state LongitudinalTrainer_optimizer_epoch80.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\\states\n",
      "Progress stored in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\n",
      "Ending epoch 81, loss 0.4998226761817932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [00:09<00:01,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90\n",
      "Train CrossEntropyLoss: 0.466 B-Acc.: 0.717 F1: 0.715\n",
      "Test CrossEntropyLoss: 0.458 B-Acc.: 0.657 F1: 0.658\n",
      "Ending epoch 91, loss 0.4623362310230732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n",
      "Epoch 100\n",
      "Train CrossEntropyLoss: 0.448 B-Acc.: 0.722 F1: 0.730\n",
      "Test CrossEntropyLoss: 0.442 B-Acc.: 0.694 F1: 0.726\n",
      "Saved PyTorch model state LSTM_epoch100.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\models\n",
      "Saved trainer state LongitudinalTrainer_optimizer_epoch100.pth in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\\states\n",
      "Progress stored in C:\\Users\\camgonza\\Box\\Camila Gonzalez's Files\\Data\\NCANDA\\NCANDA_experiments\\long_testground\\trainer\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "nr_epochs = 100\n",
    "trainer.train(model, dataloaders['Train'], dataloaders, \n",
    "              nr_epochs=nr_epochs, starting_from_epoch=0,\n",
    "              print_loss_every=int(nr_epochs/10), eval_every=int(nr_epochs/10), export_every=int(nr_epochs/5), verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
